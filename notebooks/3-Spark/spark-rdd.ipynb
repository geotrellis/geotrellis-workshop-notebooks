{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ca3df2-cad2-4e16-8768-bfca026bb576",
   "metadata": {},
   "source": [
    "# Spark RDDs\n",
    "This notebook is intended to teach you the basic structure of Apache Spark, from the basic structures to some of the more advanced techniques.  We'll do our best to give you practical advice, and not get bogged down in little minutia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e323ed5-86bc-48aa-a85d-4c53d546d1d0",
   "metadata": {},
   "source": [
    "## Initializing Spark\n",
    "We must begin by making sure that the needed JARs are present on the system, and that the interpreter knows we're loading them into the classpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88193e7-7e15-4c0e-acc9-e0ab458297ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.logging.log4j:log4j-core:2.17.0`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-1.2-api:2.17.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.3.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b1bc1-e27a-489f-97c0-a2884978f841",
   "metadata": {},
   "source": [
    "As a convenience, let's also quiet the logging facility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcb3ca-1ce2-41e4-96e8-2e091a18cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger.setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117ab1e-e308-40c1-b068-accd83761994",
   "metadata": {},
   "source": [
    "Next, we can start a `SparkSession`; this object manages the interaction between the Spark execution engine and this interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc091b-1385-42f2-99c8-a83f610cf913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[4]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18596cc8-386d-4075-872f-722f6fd20930",
   "metadata": {},
   "source": [
    "Because we're going to begin our introduction to Spark by using the older RDD interface, we're going to make available the `SparkContext` that is wrapped by `spark`.  We're going to declare this as an `implicit val` because some functions use an implicit argument to access it to keep us from needing to pass the value explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b52a2d-8df1-4325-b4b7-21dc914aa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit val sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8de4369b-a5b6-4e96-a9bb-1e540778400f",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "The heart and soul of Spark is the RDD.  This is an immutable, distributed data structure, where the data are spread across Spark's worker nodes (also called _executors_).  It is also fault tolerant (hence the R in RDD), in the fact that Spark maintains a call graph for each partition, so if an executor is lost, the partition can be shifted to another node and recalculated.\n",
    "\n",
    "The RDD is the base level structure in Spark, and provides a host of methods for manipulating the contained data.  Most of these functions take on a functional flavor, particularly due to the immutability of the RDD.  Therefore, our code will appear to use a sequence of _transformations_ of a base RDD (often loaded from a remote data store) to generate the result we need.\n",
    "\n",
    "### Creating RDDs from scratch\n",
    "\n",
    "If you have programmatically-generated data that is small enough to fit entirely in driver memory, you can use the `parallelize` method of the `SparkContext`.  Note that this will not work for very large data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f4699-54b8-400b-b119-6d8c3779f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sequenceRDD = sc.parallelize(Range(1, 10000).toSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7616011-7b8a-4311-aa5a-01e766afb310",
   "metadata": {},
   "source": [
    "There are also built-in methods for loading text from local files (`textFile`) or loading files from the Hadoop file system (`hadoopFile`).  It will be up to the user to convert these inputs into something useful.\n",
    "\n",
    "Given an RDD, it is then our job to construct a collection of [transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) that convert the input data into a useful output, which could be a single value or a sequence.  This chain of transformations constructs an internal call graph in the Spark execution engine which is lazily computed.  Only as much data as are needed will be pulled in to achieve the required result.  There are a smaller number of [actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions) which actually trigger computations.\n",
    "\n",
    "As an example, the above RDD can be filtered to hold just the values below a threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d929a-d8d9-4550-9562-16f94cd85bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val filtered = sequenceRDD.filter{ x => x < 5000 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0ee9c-ce51-4507-af93-df3a197c489f",
   "metadata": {},
   "source": [
    "Here, we only _describe_ a computation.  To do something with it, we have to call an action.  We can try a reduce operation to find the sum of the elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24123840-b341-4e4e-ab2d-836994d21363",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd62cd0-df4c-4c13-bbd3-168fda0f3977",
   "metadata": {},
   "source": [
    "We could have made a more complex chain of operations prior to this reduce step to generate a more interesting result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff16c8-6c25-4361-91df-2d07a0dd5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.log\n",
    "val filteredAndMapped = filtered.map{ x => log(x) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea32c6-a2b3-47ad-b93c-2e48fba21542",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredAndMapped.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16235f39-fb0e-42bd-adb3-d9becb9e6457",
   "metadata": {},
   "source": [
    "Note that nothing was stopping us from mapping and then filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5aa0b4-c6cb-4966-b8c4-93f21870dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceRDD.map{ x => log(x) }.filter(_ < log(5000)).reduce(_ + _)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92f915ac-7b2d-4b84-b9e4-5192be91a15d",
   "metadata": {},
   "source": [
    "Thus, users of the RDD Spark interface must take care to choose an optimal computation path, or excess computation will occur.\n",
    "\n",
    "## A Geotrellis example\n",
    "\n",
    "It's fairly limiting to work with synthetic data and contrived mappings, so we're going to look at a more complicated example that is only \n",
    "\n",
    "### Loading RDDs from a file\n",
    "\n",
    "Let's load a dataset that we can mess around with.  The dataset we've chosen is an extract of parcel information from the city of Philadelphia.  The data are formatted in line-delimited GeoJSON, so each line is a GeoJSON feature containing a JTS Polygon with a map of additional information.  There are not a wide array of options for loading data into RDDs, but in this case, we can use the `textFile()` method on the `SparkContext`, which creates an RDD where each element is a line from the source text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b90396-27eb-4ecf-96ec-0cd99cea9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "val parcel_raw = sc.textFile(\"data/extract.geojson.ld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b2625-912e-4d5a-9c3a-3544454830ed",
   "metadata": {},
   "source": [
    "At the moment, we have an `RDD[String]`, which is not very useful.  For this demonstration, we'd like to pull only the geometries from these records.  This requires some new imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6e6e0-0d2d-4f65-8330-7fedf5567ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.locationtech.geotrellis::geotrellis-vector:3.6.3`\n",
    "\n",
    "import geotrellis.vector._\n",
    "import geotrellis.vector.io._\n",
    "import geotrellis.vector.io.json._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c196b6-8711-4c02-a681-7bba3278a799",
   "metadata": {},
   "source": [
    "These imports bring some elements of Circe, the Scala JSON parsing library, into scope, so now we can see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cabbb2-77e0-4b35-b4e5-fcdbd7ae07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(parcel_raw.first.parseJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ed80d-e780-4d33-a1ad-8863e570a32f",
   "metadata": {},
   "source": [
    "A note that we used the RDD's `first` method which triggered a simple computation.  We can observe the tracking information provided by Spark at https://localhost:4040 (if you didn't redirect to a different port when you started the container).\n",
    "\n",
    "### Laziness and Persistence\n",
    "\n",
    "The above string can be parsed and converted to a Polygon using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66a318-b7c8-4dd5-8a31-ba6045983fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToPolygon(s: String): Polygon = {\n",
    "    val parsed = s.parseJson\n",
    "    parsed.hcursor.downField(\"geometry\").as[Polygon].getOrElse(GeomFactory.factory.createPolygon())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3a41e-bb82-4aa6-943d-2de82b61efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val parcels = parcel_raw.map(convertToPolygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6e759-3bc1-45c1-8213-fb23db1a91de",
   "metadata": {},
   "source": [
    "This application of `map` will not trigger any work, due to Spark's laziness, which we discussed earlier.  Confirm this by looking at the Spark UI; there should be no new jobs listed.  The `parcels` RDD is presently a description of work to be done.  Once a triggering action is called, this object becomes realized, but, Spark is not guaranteed to hold onto the contents of `parcels`.  This is not a problem unless we intend to use `parcels` as the base of more than one computation.  If we do, we might trigger multiple recomputations of the same RDD.  This can be prevented by calling `cache()` or `persist()` on the RDD of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b9289-bb90-47d5-8132-c2a40ec3e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK)\n",
    "// parcels.cache()   // Equivalent to persisting to StorageLevel.MEMORY_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937677d-af22-4824-821d-07774d5a28a3",
   "metadata": {},
   "source": [
    "Persisting an RDD will add it to the cluster's storage, and there are a variety of [storage modes](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/storage/StorageLevel.html) to choose from.  We must take care not to overuse this mechanism, lest we run out of space on our nodes, but it can significantly speed up some workflows.\n",
    "\n",
    "After calling persist on this RDD, if we go to the Storage tab in the Spark UI, we'll see nothing, because we still have not executed the code to build the lazy RDD.  But if we do something with `parcels`, it should appear.  So, let's trigger a fairly innocuous action of grabbing the first element of the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5808a7-7fcb-49bc-8345-ed45cc3a400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182409b-ab2b-45fe-b0b7-d66767d1c8fe",
   "metadata": {},
   "source": [
    "Refreshing the Storage tab should show that we did some work and stashed it for later, but because the work we did only needed a single element, we didn't compute the whole dataset.\n",
    "\n",
    "We can compute an aggregate area for all the parcels, which will use the whole dataset, caching the remaining blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6c3c9-9cd5-402d-ab47-2dc99aa9b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.map(_.reproject(geotrellis.proj4.LatLng, geotrellis.proj4.WebMercator).getArea).fold(0.0)(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2546acf-0e10-4771-98d5-effbd4613760",
   "metadata": {},
   "source": [
    "With that done, we can unpersist the parcels, since we don't need it stored any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d0904-ca05-48a8-9869-5cfba3190d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.unpersist(true) // The true says to block until the storage is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc686f9e-7117-4833-ac3d-99cb774a67ec",
   "metadata": {},
   "source": [
    "### Paired RDD Operations\n",
    "\n",
    "Many interesting operations with RDDs require that we key our data.  Once keyed, we can group records to compute aggregated values.  This would be the way to figure out, for example, how much area in Philadelphia is devoted to which zoning type (if we had zoning information for each parcel): key by zone type and aggregate by summing areas per zone.  In our case, we're going to create a mask raster for our sample of parcels by\n",
    "\n",
    "1. keying each parcel to a grid cell for some layout,\n",
    "2. rasterizing each parcel to a raster chip,\n",
    "3. merging all the parcel rasters for a given grid cell, and\n",
    "4. stitching the results into a final raster.\n",
    "\n",
    "In actual practice, steps 2 and 3 will be combined, so this won't be as inefficient as it sounds.  The total size of the computation will be dominated by the number of grid cells and the resolution of each chip.\n",
    "\n",
    "We're going to use some additional Geotrellis facilities for this, so let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b163e-2d4f-401c-9b31-52d3ffdd2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.locationtech.geotrellis::geotrellis-raster:3.6.3`\n",
    "import $ivy.`org.locationtech.geotrellis::geotrellis-layer:3.6.3`\n",
    "\n",
    "import geotrellis.raster._\n",
    "import geotrellis.layer._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7dfaa-7761-45b3-97b0-e4878183b25b",
   "metadata": {},
   "source": [
    "To key each parcel to a grid, we're going to need to establish a layout.  We'll use the Geotrellis layout scheme that corresponds to the power-of-two pyramid often used for web maps.  The grid that we'll use for this exercise will be defined at a fixed zoom level.  The finer the zoom, the more cells, and therefore, keys we'll have to work with, but also the bigger the final image that we'll produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66fd96-a224-46d7-a874-93644a8d8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val zoom = 14\n",
    "val LayoutLevel(_, layout) = ZoomedLayoutScheme(geotrellis.proj4.WebMercator).levelForZoom(zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfec9b-740c-420d-a4ec-2a166faef207",
   "metadata": {},
   "source": [
    "Let's get some idea of the scale of the problem that we're going to be looking at.  For starters, how many keys will we be working with?\n",
    "\n",
    "To make these calculations, it will be important to make sure that our parcel boundaries are in the correct projection.  Then, we can figure out the extent of the whole dataset and determine how many grid cells are going to participate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b99b6-8693-4f42-8bd5-bd7195e5b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wmParcels = parcels.map(_.reproject(geotrellis.proj4.LatLng, geotrellis.proj4.WebMercator))\n",
    "val totalExtent = wmParcels.map(_.extent).reduce(_.combine(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572ae23-1e2f-4a51-90eb-d121e130f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "val mapTransform = layout.mapTransform\n",
    "val regionBounds = mapTransform.extentToBounds(totalExtent)\n",
    "// The following will compute the upper bound of the number of grid cells\n",
    "// Not every cell is guaranteed to have a member\n",
    "println(f\"Upper bound on the count of cells: ${regionBounds.coordsIter.length} (grid region dimensions: ${(regionBounds.width, regionBounds.height)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6099f-289b-422f-821b-f2778f734a81",
   "metadata": {},
   "source": [
    "Given this precalculation, we can infer that, as long as the individual chips that we use in our layout aren't too big, we can make a reasonably-sized final image.  \n",
    "\n",
    "Let's proceed with assigning a key to each parcel.  Note, however, that some parcels will cross grid cell boundaries, so we have to produce a sequence of keys for each parcel and merge all the results.  This is the role of a `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488af52-aca6-4324-b887-acc7e7f4873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyedParcels = wmParcels.flatMap{ parcel =>\n",
    "    mapTransform.keysForGeometry(parcel).map{ k => (k, (k, parcel)) }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a69a18-0747-4544-9f68-4b0594fd1fdf",
   "metadata": {},
   "source": [
    "For reasons that will be clear below, we need to replicate the grid cell location in both the key and value portion of the paired RDD.\n",
    "\n",
    "### Partitions and Shuffling\n",
    "\n",
    "Spark distributes data across a set of worker nodes, called _executors_.  An RDD is comprised of _partitions_, with the partitions being spread across the cluster's executors.  There is no requirement for the partitions to be of equal size, so _partition skew_ is more than possible, and this means that some executors will have more work to do than others because the data are distributed heterogeneously, and some unevenness is likely.\n",
    "\n",
    "We can do things to make this problem better or worse.  If the problem set has, like ours, an inherently non-uniform distribution of data (some geographical areas are more densely populated), then grouping by key is likely to produce skewed partitions.  So, even though this is the logical first step, it's better to just work on the data where they are, and pass smaller, intermediate results between executors as we combine.\n",
    "\n",
    "For this problem, the intermediate data are the small raster chips that we're going to rasterize our footprints to.  These will have a fixed and relatively small size, while the number of raw data elements to be shuffled in a group by key could be much, much larger.\n",
    "\n",
    "Considering the ways to make data distributions better, choosing a good number of partitions is one thing we can do.  Let's see how many we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243fe0a-f304-45c5-903d-a59dfe19d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(keyedParcels.partitions.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58890ad4-68f4-4acd-ab8d-d03a89023b2b",
   "metadata": {},
   "source": [
    "Depending on the number of workers, this may be a good or bad number of partitions.  We might target some number of partitions per executor, but there is also a management penalty for having too many, which is borne by the master node as additional memory and processing overhead.  We'll increase the number of partitions here just to show how it is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c421091-8739-48a2-97a5-f928d6987a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyedParcelsRP = keyedParcels.repartition(keyedParcels.partitions.length * 4)\n",
    "println(keyedParcelsRP.partitions.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eae6a5-3fc2-4cd7-babf-2bcb099e2c44",
   "metadata": {},
   "source": [
    "### Completing the exercise\n",
    "\n",
    "Now, we can go about doing the conversion from parcel geometries to mask rasters.  The approach will be to use [`combineByKey`](https://spark.apache.org/docs/0.7.3/api/core/spark/PairRDDFunctions.html), which essentially folds per-key, per-partition, shuffles the intermediate results, and then merges them to get the final result per key.  This requires defining an initial value, explaining how to add a value to it, and providing a means to merge the intermediates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62877b-c94f-4f2d-9fa2-988bd7da38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotrellis.raster.rasterize._\n",
    "\n",
    "// Define the chip dimensions\n",
    "val chipSize = 128\n",
    "\n",
    "def burnParcel(parcel: Polygon, key: SpatialKey, tile: BitArrayTile): Unit = {\n",
    "    val rasterExtent = RasterExtent(mapTransform.keyToExtent(key), chipSize, chipSize)\n",
    "    parcel.foreach(rasterExtent){ (r: Int, c:Int) => tile.set(r, c, 1) }\n",
    "}\n",
    "\n",
    "def createFromParcel(keyedParcel: (SpatialKey, Polygon)): BitArrayTile = {\n",
    "    val tile = BitArrayTile.empty(chipSize, chipSize)\n",
    "    val (key, parcel) = keyedParcel\n",
    "    burnParcel(parcel, key, tile)\n",
    "    tile\n",
    "}\n",
    "\n",
    "def addParcelToTile(tile: BitArrayTile, keyedParcel: (SpatialKey, Polygon)): BitArrayTile = {\n",
    "    val (key, parcel) = keyedParcel\n",
    "    burnParcel(parcel, key, tile)\n",
    "    tile\n",
    "}\n",
    "\n",
    "def mergeTiles(tile1: BitArrayTile, tile2: BitArrayTile): BitArrayTile =\n",
    "    tile1.combine(tile2){ (v1, v2) => if (v1 + v2 > 0) 1 else 0 }.asInstanceOf[BitArrayTile]\n",
    "\n",
    "val tileRDD = keyedParcelsRP.combineByKey(createFromParcel, addParcelToTile, mergeTiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29246b11-4641-469d-9dfd-cdcc72461b83",
   "metadata": {},
   "source": [
    "At this point, we have a tile per non-empty SpatialKey.  We need to assemble the result into a complete raster.  There are a number of ways one could conceive of doing this, but fortunately, Geotrellis already offers [stitching](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/stitch/StitchRDDMethods.scala) extension methods.  The only thing we need to do is satisfy the base type requirement of `RDD[(SpatialKey, T)] with Metadata[M]`, where `V` is a compatible tile type and `M` carries a layout definition.  For this reason, Geotrellis provides the `ContextRDD` and `TileLayerMetadata` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3ec18-a687-48dd-9875-97c4f7d52b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.locationtech.geotrellis::geotrellis-spark:3.6.3`\n",
    "import geotrellis.raster.render._\n",
    "import geotrellis.spark._\n",
    "import geotrellis.spark.stitch._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684efd7d-4fbd-4e7f-b043-9977b362780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val metadata = TileLayerMetadata(\n",
    "    BitCellType, \n",
    "    layout, \n",
    "    mapTransform.boundsToExtent(regionBounds), \n",
    "    geotrellis.proj4.WebMercator, \n",
    "    KeyBounds(regionBounds)\n",
    ")\n",
    "val tilesWithMetadata = ContextRDD(tileRDD, metadata).asInstanceOf[ContextRDD[SpatialKey, Tile, TileLayerMetadata[SpatialKey]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3ed6c-eade-48f2-9254-3a2726967367",
   "metadata": {},
   "outputs": [],
   "source": [
    "tilesWithMetadata\n",
    "  .stitch\n",
    "  .tile\n",
    "  .renderPng(ColorMap(Map(0 -> RGB(0,0,0), 1->RGB(255,255,0))))\n",
    "  .write(\"test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6664cf-7328-4255-91a9-216e6aa50bdf",
   "metadata": {},
   "source": [
    "### A note about shuffling and serialization\n",
    "\n",
    "It's the case that when Spark sends objects over the wire from executor to another executor or the driver, these objects need to be converted into a byte stream.  That stream needs to be generated somehow, but how that happens matters for performance.  Spark can use either plain old Java serialization or it can use Kryo serialization.  The former is more ubiquitous, the latter more performant.  In order to use the latter, one must configure the spark session to use it.  There is a Spark configuration field `spark.kryo.registrator` that can be set to `classOf[KryoRegistrator].getName`, but in that case, it is best to also set `spark.kryo.registrationRequired` to false, to avoid serialization errors.  Serialization errors are often not generated when using a local master, so testing new code on a genuine, multi-node cluster is advisible before deploying code.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "1. Generate some random `(x, y)` pairs of Doubles over the unit square.\n",
    "2. Grid up the unit square into a number of cells, and assign each point a key based on the cell it falls inside.\n",
    "3. Compute the average of the points in each cell.\n",
    "4. Compute the minimum, maximum, and average distance between the cell centers and the per-cell average\n",
    "\n",
    "Parameterize this process, and see how the deviations change as the number of points increases and the number of grid cells increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe736fa-90f3-4065-a862-e46a0f59c85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b43732f-f14c-414a-9e01-1b1a801e1ab9",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "To ensure that we aren't holding the Spark UI port, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b460d4-e372-4665-83a1-42f73bd3e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
