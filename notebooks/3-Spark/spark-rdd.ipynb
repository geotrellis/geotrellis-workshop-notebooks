{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ca3df2-cad2-4e16-8768-bfca026bb576",
   "metadata": {},
   "source": [
    "# Spark RDDs\n",
    "This notebook is intended to teach you the basic structure of Apache Spark, from the basic structures to some of the more advanced techniques.  We'll do our best to give you practical advice, and not get bogged down in little minutia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e323ed5-86bc-48aa-a85d-4c53d546d1d0",
   "metadata": {},
   "source": [
    "## Initializing Spark\n",
    "We must begin by making sure that the needed JARs are present on the system, and that the interpreter knows we're loading them into the classpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88193e7-7e15-4c0e-acc9-e0ab458297ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.logging.log4j:log4j-core:2.17.0`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-1.2-api:2.17.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.3.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b1bc1-e27a-489f-97c0-a2884978f841",
   "metadata": {},
   "source": [
    "As a convenience, let's also quiet the logging facility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2fcb3ca-1ce2-41e4-96e8-2e091a18cf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger.setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117ab1e-e308-40c1-b068-accd83761994",
   "metadata": {},
   "source": [
    "Next, we can start a `SparkSession`; this object manages the interaction between the Spark execution engine and this interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dc091b-1385-42f2-99c8-a83f610cf913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Loading <code>spark-stubs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Getting spark JARs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Creating SparkSession\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://2e4239bf6209:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@60d1ac27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[4]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18596cc8-386d-4075-872f-722f6fd20930",
   "metadata": {},
   "source": [
    "Because we're going to begin our introduction to Spark by using the older RDD interface, we're going to make available the `SparkContext` that is wrapped by `spark`.  We're going to declare this as an `implicit val` because some functions use an implicit argument to access it to keep us from needing to pass the value explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b52a2d-8df1-4325-b4b7-21dc914aa29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@59b7590b"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8de4369b-a5b6-4e96-a9bb-1e540778400f",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "The heart and soul of Spark is the RDD.  This is an immutable, distributed data structure, where the data are spread across Spark's worker nodes (also called _executors_).  It is also fault tolerant (hence the R in RDD), in the fact that Spark maintains a call graph for each partition, so if an executor is lost, the partition can be shifted to another node and recalculated.\n",
    "\n",
    "The RDD is the base level structure in Spark, and provides a host of methods for manipulating the contained data.  Most of these functions take on a functional flavor, particularly due to the immutability of the RDD.  Therefore, our code will appear to use a sequence of _transformations_ of a base RDD (often loaded from a remote data store) to generate the result we need.\n",
    "\n",
    "Let's load a dataset that we can mess around with to start.  The dataset we've chosen is an extract of parcel information from the city of Philadelphia.  The data are formatted in line-delimited GeoJSON, so each line is a GeoJSON feature containing a JTS Polygon with a map of additional information.  There are not a wide array of options for loading data into RDDs, but in this case, we can use the `textFile()` method on the `SparkContext`, which creates an RDD where each element is a line from the source text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b90396-27eb-4ecf-96ec-0cd99cea9e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mparcel_raw\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = data/extract.geojson.ld MapPartitionsRDD[1] at textFile at cell5.sc:1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parcel_raw = sc.textFile(\"data/extract.geojson.ld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b2625-912e-4d5a-9c3a-3544454830ed",
   "metadata": {},
   "source": [
    "At the moment, we have an RDD of `String` elements, which are not very useful.  For this demonstration, we'd like to pull only the geometries from these records.  This requires some new imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d6e6e0-0d2d-4f65-8330-7fedf5567ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.vector._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.vector.io._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.vector.io.json._\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.locationtech.geotrellis::geotrellis-vector:3.6.3`\n",
    "\n",
    "import geotrellis.vector._\n",
    "import geotrellis.vector.io._\n",
    "import geotrellis.vector.io.json._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c196b6-8711-4c02-a681-7bba3278a799",
   "metadata": {},
   "source": [
    "These imports bring some elements of Circe, the Scala JSON parsing library, into scope, so now we can see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cabbb2-77e0-4b35-b4e5-fcdbd7ae07dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\" : \"Feature\",\n",
      "  \"properties\" : {\n",
      "    \"OBJECTID\" : 583279,\n",
      "    \"RECSUB\" : null,\n",
      "    \"BASEREG\" : \"073N150045\",\n",
      "    \"MAPREG\" : \"073N150045\",\n",
      "    \"PARCEL\" : \"0045\",\n",
      "    \"RECMAP\" : \"073N15\",\n",
      "    \"STCOD\" : 88820,\n",
      "    \"HOUSE\" : 607,\n",
      "    \"SUF\" : null,\n",
      "    \"UNIT\" : null,\n",
      "    \"STEX\" : null,\n",
      "    \"STDIR\" : \"N\",\n",
      "    \"STNAM\" : \"52ND\",\n",
      "    \"STDESSUF\" : null,\n",
      "    \"ELEV_FLAG\" : 0,\n",
      "    \"TOPELEV\" : 9999.0,\n",
      "    \"BOTELEV\" : -9999.0,\n",
      "    \"CONDOFLAG\" : 0,\n",
      "    \"MATCHFLAG\" : 1,\n",
      "    \"INACTDATE\" : \"1899-12-30\",\n",
      "    \"ORIG_DATE\" : \"2003-02-07\",\n",
      "    \"STATUS\" : 1,\n",
      "    \"GEOID\" : null,\n",
      "    \"STDES\" : \"ST\",\n",
      "    \"ADDR_SOURC\" : \"607 N 52ND ST\",\n",
      "    \"ADDR_STD\" : \"607 N 52ND ST\",\n",
      "    \"COMMENTS\" : null,\n",
      "    \"PIN\" : 1001657538,\n",
      "    \"FRAC\" : null,\n",
      "    \"UNIT_TYPE\" : null,\n",
      "    \"STEX_FRAC\" : null,\n",
      "    \"STEX_SUF\" : null,\n",
      "    \"SEPARATED_\" : null,\n",
      "    \"MUNIMENT_T\" : null,\n",
      "    \"MUNIMENT_I\" : null,\n",
      "    \"DOR_REVIEW\" : null,\n",
      "    \"OPA_REVIEW\" : null,\n",
      "    \"PWD_REVIEW\" : null,\n",
      "    \"Shape__Are\" : 200.484375,\n",
      "    \"Shape__Len\" : 75.561137227720707\n",
      "  },\n",
      "  \"geometry\" : {\n",
      "    \"type\" : \"Polygon\",\n",
      "    \"coordinates\" : [\n",
      "      [\n",
      "        [\n",
      "          -75.2252876,\n",
      "          39.9697445\n",
      "        ],\n",
      "        [\n",
      "          -75.2250114,\n",
      "          39.9697865\n",
      "        ],\n",
      "        [\n",
      "          -75.2250214,\n",
      "          39.9698309\n",
      "        ],\n",
      "        [\n",
      "          -75.2252978,\n",
      "          39.9697867\n",
      "        ],\n",
      "        [\n",
      "          -75.2252876,\n",
      "          39.9697445\n",
      "        ]\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(parcel_raw.first.parseJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ed80d-e780-4d33-a1ad-8863e570a32f",
   "metadata": {},
   "source": [
    "A note that we used the RDD's `first` method which triggered a simple computation.  We can observe the tracking information provided by Spark at https://localhost:4040 (if you didn't redirect to a different port when you started the container).\n",
    "\n",
    "### Mapping, Laziness, and Persistence\n",
    "\n",
    "The above string can be parsed and converted to a Polygon using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f66a318-b7c8-4dd5-8a31-ba6045983fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mconvertToPolygon\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertToPolygon(s: String): Polygon = {\n",
    "    val parsed = s.parseJson\n",
    "    parsed.hcursor.downField(\"geometry\").as[Polygon].getOrElse(GeomFactory.factory.createPolygon())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a3a41e-bb82-4aa6-943d-2de82b61efe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mparcels\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mPolygon\u001b[39m] = MapPartitionsRDD[2] at map at cell9.sc:1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parcels = parcel_raw.map(convertToPolygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6e759-3bc1-45c1-8213-fb23db1a91de",
   "metadata": {},
   "source": [
    "This application of `map` will not trigger any work.  Confirm this by looking at the Spark UI; there should be no new jobs listed.  Spark is _lazy_, and so won't actually do work until it's truly needed.  The `parcels` RDD is presently a description of work to be done.  Once a triggering action is called, this object becomes realized.  However, Spark is not guaranteed to hang on to the contents of `parcels` unless we call `cache()` or `persist()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9b9289-bb90-47d5-8132-c2a40ec3e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mPolygon\u001b[39m] = MapPartitionsRDD[2] at map at cell9.sc:1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK)\n",
    "// parcels.cache()   // Equivalent to persisting to StorageLevel.MEMORY_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937677d-af22-4824-821d-07774d5a28a3",
   "metadata": {},
   "source": [
    "If we go to the Storage tab in the Spark UI, we'll see nothing, because we still have not executed the code to build the lazy RDD.  But if we do something with `parcels`, it should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5808a7-7fcb-49bc-8345-ed45cc3a400e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mPolygon\u001b[39m = POLYGON ((-75.2252876 39.9697445, -75.2250114 39.9697865, -75.2250214 39.9698309, -75.2252978 39.9697867, -75.2252876 39.9697445))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels.first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182409b-ab2b-45fe-b0b7-d66767d1c8fe",
   "metadata": {},
   "source": [
    "Refreshing the Storage tab should show that we did some work and stashed it for later, but because the work we did only needed a single element, we didn't compute the whole dataset.\n",
    "\n",
    "We can compute an aggregate area for all the parcels, which will use the whole dataset, caching the remaining blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb6c3c9-9cd5-402d-ab47-2dc99aa9b115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m805033.5802057029\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels.map(_.reproject(geotrellis.proj4.LatLng, geotrellis.proj4.WebMercator).getArea).fold(0.0)(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2546acf-0e10-4771-98d5-effbd4613760",
   "metadata": {},
   "source": [
    "With that done, we can unpersist the parcels, since we don't need it stored any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb3d0904-ca05-48a8-9869-5cfba3190d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mPolygon\u001b[39m] = MapPartitionsRDD[2] at map at cell9.sc:1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels.unpersist(true) // The true says to block until the storage is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc686f9e-7117-4833-ac3d-99cb774a67ec",
   "metadata": {},
   "source": [
    "### Paired RDD Operations\n",
    "\n",
    "Many interesting operations with RDDs require that we key our data.  Once keyed, we can group records to compute aggregated values.  This would be the way to figure out, for example, how much area in Philadelphia is devoted to which zoning type (if we had zoning information for each parcel): key by zone type and aggregate by summing areas per zone.  In our case, we're going to create a mask raster for our sample of parcels by\n",
    "\n",
    "1. keying each parcel to a grid cell for some layout,\n",
    "2. rasterizing each parcel to a raster chip,\n",
    "3. merging all the parcel rasters for a given grid cell, and\n",
    "4. stitching the results into a final raster.\n",
    "\n",
    "In actual practice, steps 2 and 3 will be combined, so this won't be as inefficient as it sounds.  The total size of the computation will be dominated by the number of grid cells and the resolution of each chip.\n",
    "\n",
    "We're going to use some additional Geotrellis facilities for this, so let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "033b163e-2d4f-401c-9b31-52d3ffdd2836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.raster._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.layer._\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.locationtech.geotrellis::geotrellis-raster:3.6.3`\n",
    "import $ivy.`org.locationtech.geotrellis::geotrellis-layer:3.6.3`\n",
    "\n",
    "import geotrellis.raster._\n",
    "import geotrellis.layer._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7dfaa-7761-45b3-97b0-e4878183b25b",
   "metadata": {},
   "source": [
    "To key each parcel to a grid, we're going to need to establish a layout.  We'll use the Geotrellis layout scheme that corresponds to the power-of-two pyramid often used for web maps.  The grid that we'll use for this exercise will be defined at a fixed zoom level.  The finer the zoom, the more cells, and therefore, keys we'll have to work with, but also the bigger the final image that we'll produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af66fd96-a224-46d7-a874-93644a8d8fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mzoom\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m14\u001b[39m\n",
       "\u001b[36mlayout\u001b[39m: \u001b[32mLayoutDefinition\u001b[39m = \u001b[33mLayoutDefinition\u001b[39m(\n",
       "  \u001b[33mExtent\u001b[39m(\n",
       "    \u001b[32m-2.0037508342789244E7\u001b[39m,\n",
       "    \u001b[32m-2.0037508342789244E7\u001b[39m,\n",
       "    \u001b[32m2.0037508342789244E7\u001b[39m,\n",
       "    \u001b[32m2.0037508342789244E7\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTileLayout\u001b[39m(\u001b[32m16384\u001b[39m, \u001b[32m16384\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zoom = 14\n",
    "val LayoutLevel(_, layout) = ZoomedLayoutScheme(geotrellis.proj4.WebMercator).levelForZoom(zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfec9b-740c-420d-a4ec-2a166faef207",
   "metadata": {},
   "source": [
    "Let's get some idea of the scale of the problem that we're going to be looking at.  For starters, how many keys will we be working with?\n",
    "\n",
    "To make these calculations, it will be important to make sure that our parcel boundaries are in the correct projection.  Then, we can figure out the extent of the whole dataset and determine how many grid cells are going to participate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "120b99b6-8693-4f42-8bd5-bd7195e5b221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwmParcels\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mPolygon\u001b[39m] = MapPartitionsRDD[4] at map at cell16.sc:1\n",
       "\u001b[36mtotalExtent\u001b[39m: \u001b[32mExtent\u001b[39m = \u001b[33mExtent\u001b[39m(\n",
       "  \u001b[32m-8379409.705110261\u001b[39m,\n",
       "  \u001b[32m4848479.142425085\u001b[39m,\n",
       "  \u001b[32m-8344579.561893152\u001b[39m,\n",
       "  \u001b[32m4885580.742383389\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wmParcels = parcels.map(_.reproject(geotrellis.proj4.LatLng, geotrellis.proj4.WebMercator))\n",
    "val totalExtent = wmParcels.map(_.extent).reduce(_.combine(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0572ae23-1e2f-4a51-90eb-d121e130f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper bound on the count of cells: 240 (grid region dimensions: (15,16))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmapTransform\u001b[39m: \u001b[32mMapKeyTransform\u001b[39m = geotrellis.layer.MapKeyTransform@3cacb847\n",
       "\u001b[36mregionBounds\u001b[39m: \u001b[32mTileBounds\u001b[39m = \u001b[33mGridBounds\u001b[39m(\u001b[32m4766\u001b[39m, \u001b[32m6194\u001b[39m, \u001b[32m4780\u001b[39m, \u001b[32m6209\u001b[39m)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mapTransform = layout.mapTransform\n",
    "val regionBounds = mapTransform.extentToBounds(totalExtent)\n",
    "// The following will compute the upper bound of the number of grid cells\n",
    "// Not every cell is guaranteed to have a member\n",
    "println(f\"Upper bound on the count of cells: ${regionBounds.coordsIter.length} (grid region dimensions: ${(regionBounds.width, regionBounds.height)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6099f-289b-422f-821b-f2778f734a81",
   "metadata": {},
   "source": [
    "Given this precalculation, we can infer that, as long as the individual chips that we use in our layout aren't too big, we can make a reasonably-sized final image.  \n",
    "\n",
    "Let's proceed with assigning a key to each parcel.  Note, however, that some parcels will cross grid cell boundaries, so we have to produce a sequence of keys for each parcel and merge all the results.  This is the role of a `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7488af52-aca6-4324-b887-acc7e7f4873c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mkeyedParcels\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mSpatialKey\u001b[39m, (\u001b[32mSpatialKey\u001b[39m, \u001b[32mPolygon\u001b[39m))] = MapPartitionsRDD[6] at flatMap at cell18.sc:1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val keyedParcels = wmParcels.flatMap{ parcel =>\n",
    "    mapTransform.keysForGeometry(parcel).map{ k => (k, (k, parcel)) }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a69a18-0747-4544-9f68-4b0594fd1fdf",
   "metadata": {},
   "source": [
    "For reasons that will be clear below, we need to replicate the grid cell location in both the key and value portion of the paired RDD.\n",
    "\n",
    "### Partitions and Shuffling\n",
    "\n",
    "Spark distributes data across a set of worker nodes, called _executors_.  An RDD is comprised of _partitions_, with the partitions being spread across the cluster's executors.  There is no requirement for the partitions to be of equal size, so _partition skew_ is more than possible, and this means that some executors will have more work to do than others because the data are distributed heterogeneously, and some unevenness is likely.\n",
    "\n",
    "We can do things to make this problem better or worse.  If the problem set has, like ours, an inherently non-uniform distribution of data (some geographical areas are more densely populated), then grouping by key is likely to produce skewed partitions.  So, even though this is the logical first step, it's better to just work on the data where they are, and pass smaller, intermediate results between executors as we combine.\n",
    "\n",
    "For this problem, the intermediate data are the small raster chips that we're going to rasterize our footprints to.  These will have a fixed and relatively small size, while the number of raw data elements to be shuffled in a group by key could be much, much larger.\n",
    "\n",
    "Considering the ways to make data distributions better, choosing a good number of partitions is one thing we can do.  Let's see how many we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d243fe0a-f304-45c5-903d-a59dfe19d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "println(keyedParcels.partitions.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58890ad4-68f4-4acd-ab8d-d03a89023b2b",
   "metadata": {},
   "source": [
    "Depending on the number of workers, this may be a good or bad number of partitions.  We might target some number of partitions per executor, but there is also a management penalty for having too many, which is borne by the master node as additional memory and processing overhead.  We'll increase the number of partitions here just to show how it is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c421091-8739-48a2-97a5-f928d6987a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mkeyedParcelsRP\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mSpatialKey\u001b[39m, (\u001b[32mSpatialKey\u001b[39m, \u001b[32mPolygon\u001b[39m))] = MapPartitionsRDD[10] at repartition at cell20.sc:1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val keyedParcelsRP = keyedParcels.repartition(keyedParcels.partitions.length * 4)\n",
    "println(keyedParcelsRP.partitions.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eae6a5-3fc2-4cd7-babf-2bcb099e2c44",
   "metadata": {},
   "source": [
    "Now, we can go about doing the conversion from parcel geometries to mask rasters.  The approach will be to use [`combineByKey`](https://spark.apache.org/docs/0.7.3/api/core/spark/PairRDDFunctions.html), which essentially folds per-key, per-partition, shuffles the intermediate results, and then merges them to get the final result per key.  This requires defining an initial value, explaining how to add a value to it, and providing a means to merge the intermediates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e62877b-c94f-4f2d-9fa2-988bd7da38fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.raster.rasterize._\u001b[39m\n",
       "\u001b[36mchipSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m128\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mburnParcel\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateFromParcel\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36maddParcelToTile\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmergeTiles\u001b[39m\n",
       "\u001b[36mtileRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mSpatialKey\u001b[39m, \u001b[32mBitArrayTile\u001b[39m)] = ShuffledRDD[16] at combineByKey at cell48.sc:27"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geotrellis.raster.rasterize._\n",
    "\n",
    "// Define the chip dimensions\n",
    "val chipSize = 128\n",
    "\n",
    "def burnParcel(parcel: Polygon, key: SpatialKey, tile: BitArrayTile): Unit = {\n",
    "    val rasterExtent = RasterExtent(mapTransform.keyToExtent(key), chipSize, chipSize)\n",
    "    parcel.foreach(rasterExtent){ (r: Int, c:Int) => tile.set(r, c, 1) }\n",
    "}\n",
    "\n",
    "def createFromParcel(keyedParcel: (SpatialKey, Polygon)): BitArrayTile = {\n",
    "    val tile = BitArrayTile.empty(chipSize, chipSize)\n",
    "    val (key, parcel) = keyedParcel\n",
    "    burnParcel(parcel, key, tile)\n",
    "    tile\n",
    "}\n",
    "\n",
    "def addParcelToTile(tile: BitArrayTile, keyedParcel: (SpatialKey, Polygon)): BitArrayTile = {\n",
    "    val (key, parcel) = keyedParcel\n",
    "    burnParcel(parcel, key, tile)\n",
    "    tile\n",
    "}\n",
    "\n",
    "def mergeTiles(tile1: BitArrayTile, tile2: BitArrayTile): BitArrayTile =\n",
    "    tile1.combine(tile2){ (v1, v2) => if (v1 + v2 > 0) 1 else 0 }.asInstanceOf[BitArrayTile]\n",
    "\n",
    "val tileRDD = keyedParcelsRP.combineByKey(createFromParcel, addParcelToTile, mergeTiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29246b11-4641-469d-9dfd-cdcc72461b83",
   "metadata": {},
   "source": [
    "At this point, we have a tile per non-empty SpatialKey.  We need to assemble the result into a complete raster.  There are a number of ways one could conceive of doing this, but fortunately, Geotrellis already offers [stitching](https://github.com/locationtech/geotrellis/blob/master/spark/src/main/scala/geotrellis/spark/stitch/StitchRDDMethods.scala) extension methods.  The only thing we need to do is satisfy the base type requirement of `RDD[(SpatialKey, T)] with Metadata[M]`, where `V` is a compatible tile type and `M` carries a layout definition.  For this reason, Geotrellis provides the `ContextRDD` and `TileLayerMetadata` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adb3ec18-a687-48dd-9875-97c4f7d52b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.raster.render._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.spark._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mgeotrellis.spark.stitch._\u001b[39m"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.locationtech.geotrellis::geotrellis-spark:3.6.3`\n",
    "import geotrellis.raster.render._\n",
    "import geotrellis.spark._\n",
    "import geotrellis.spark.stitch._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "684efd7d-4fbd-4e7f-b043-9977b362780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmetadata\u001b[39m: \u001b[32mTileLayerMetadata\u001b[39m[\u001b[32mSpatialKey\u001b[39m] = \u001b[33mTileLayerMetadata\u001b[39m(\n",
       "  bool,\n",
       "  \u001b[33mLayoutDefinition\u001b[39m(\n",
       "    \u001b[33mExtent\u001b[39m(\n",
       "      \u001b[32m-2.0037508342789244E7\u001b[39m,\n",
       "      \u001b[32m-2.0037508342789244E7\u001b[39m,\n",
       "      \u001b[32m2.0037508342789244E7\u001b[39m,\n",
       "      \u001b[32m2.0037508342789244E7\u001b[39m\n",
       "    ),\n",
       "    \u001b[33mTileLayout\u001b[39m(\u001b[32m16384\u001b[39m, \u001b[32m16384\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m)\n",
       "  ),\n",
       "  \u001b[33mExtent\u001b[39m(\n",
       "    \u001b[32m-8379944.284960443\u001b[39m,\n",
       "    \u001b[32m4847942.0819590185\u001b[39m,\n",
       "    \u001b[32m-8343254.511383558\u001b[39m,\n",
       "    \u001b[32m4887077.8404410295\u001b[39m\n",
       "  ),\n",
       "  WebMercator,\n",
       "  \u001b[33mKeyBounds\u001b[39m(\u001b[33mSpatialKey\u001b[39m(\u001b[32m4766\u001b[39m, \u001b[32m6194\u001b[39m), \u001b[33mSpatialKey\u001b[39m(\u001b[32m4780\u001b[39m, \u001b[32m6209\u001b[39m))\n",
       ")\n",
       "\u001b[36mtilesWithMetadata\u001b[39m: \u001b[32mContextRDD\u001b[39m[\u001b[32mSpatialKey\u001b[39m, \u001b[32mTile\u001b[39m, \u001b[32mTileLayerMetadata\u001b[39m[\u001b[32mSpatialKey\u001b[39m]] = ContextRDD[17] at RDD at ContextRDD.scala:32"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metadata = TileLayerMetadata(\n",
    "    BitCellType, \n",
    "    layout, \n",
    "    mapTransform.boundsToExtent(regionBounds), \n",
    "    geotrellis.proj4.WebMercator, \n",
    "    KeyBounds(regionBounds)\n",
    ")\n",
    "val tilesWithMetadata = ContextRDD(tileRDD, metadata).asInstanceOf[ContextRDD[SpatialKey, Tile, TileLayerMetadata[SpatialKey]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50a3ed6c-eade-48f2-9254-3a2726967367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tilesWithMetadata\n",
    "  .stitch\n",
    "  .tile\n",
    "  .renderPng(ColorMap(Map(0 -> RGB(0,0,0), 1->RGB(255,255,0))))\n",
    "  .write(\"test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6664cf-7328-4255-91a9-216e6aa50bdf",
   "metadata": {},
   "source": [
    "### A note about shuffling and serialization\n",
    "\n",
    "It's the case that when Spark sends objects over the wire from executor to another executor or the driver, these objects need to be converted into a byte stream.  That stream needs to be generated somehow, but how that happens matters for performance.  Spark can use either plain old Java serialization or it can use Kryo serialization.  The former is more ubiquitous, the latter more performant.  In order to use the latter, one must configure the spark session to use it.  There is a Spark configuration field `spark.kryo.registrator` that can be set to `classOf[KryoRegistrator].getName`, but in that case, it is best to also set `spark.kryo.registrationRequired` to false, to avoid serialization errors.  Serialization errors are often not generated when using a local master, so testing new code on a genuine, multi-node cluster is advisible before deploying code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
