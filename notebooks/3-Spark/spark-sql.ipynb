{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19fa2efe-5260-4f25-ac14-1d17b79eeec1",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "For some time, the better alternative to Spark RDDs has been provided by Spark SQL.  This API—built on top of RDDs—improves the efficiency and programming model for interacting with data.  Not only is the API more convenient for manipulating data with many records, it also offers what amounts to an optimizing compiler that will compute a more efficient plan for your job automatically, which is a huge benefit.  This compiler can be extended with custom types and operators.  In fact, the [RasterFrames project](github.com/locationtech/rasterframes) has already added the ability to work with Geotrellis types.\n",
    "\n",
    "This is the present and future of Spark, and is generally a better tool than the pure RDD interface.\n",
    "\n",
    "## Initializing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a180111-cb9f-4028-8156-a5cb250c808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.logging.log4j:log4j-core:2.17.0`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-1.2-api:2.17.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.3.1`\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger.setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[4]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._  // Provides a bunch of extra sugar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be2165-1b79-44dc-b812-85b2ecadb828",
   "metadata": {},
   "source": [
    "It's not necessary to extract the `SparkContext`, as we will be working directly with the `SparkSession` object here.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "Much as the RDD is the core data structure of vanilla Spark, the `Dataset` forms the core of Spark SQL.  It wraps an RDD with additional machinery and syntactic sugar that improves its usability over a plain RDD.  Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119e7ec-c83e-4785-a721-be5675d1e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "// The following line is needed to make this work in this Jupyter notebook;\n",
    "// this will not be needed for normal code.  Expect to see this workaround \n",
    "// below, since it allows us to declare case classes in the correct scope.\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Person(name: String, id: Int)\n",
    "\n",
    "val personDS = spark.createDataset(\n",
    "    Seq(Person(\"Judy\", 23), Person(\"Paul\", 19), Person(\"Arthur\", 42))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a75ee-12d3-4795-adfa-dc3980443d05",
   "metadata": {},
   "source": [
    "We now have a structure containing a set of case class instances.  This is useful on its own, but there is a hidden easter egg that we can reveal by displaying the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd93d4-5a5b-4335-9b21-da72a06cb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "personDS.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e56c8f-d749-419e-b580-7614a736cedd",
   "metadata": {},
   "source": [
    "This shows that Spark SQL understands the fields of the case class, and recognizes that these data have two columns.  We can see further that it understands the types as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9d2da-1dca-43d2-b5c6-003de32678f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "personDS.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac866d1b-8807-4b07-8d2b-007453f4fe14",
   "metadata": {},
   "source": [
    "What this provides is a table-like representation of arbitrary data, provided that there is an [`Encoder`](https://spark.apache.org/docs/3.3.1/api/scala/org/apache/spark/sql/Encoder.html), available explicitly or implicitly, that understands the type of data you are attempting to add to a Dataset.  In our initialization block, we saw\n",
    "```scala\n",
    "import spark.implicits._\n",
    "```\n",
    "which brought a host of implicit Encoders into scope that allowed us to simply add a sequence of our custom case class to a Dataset.\n",
    "\n",
    "### Dataframe conversion\n",
    "\n",
    "Datasets give us the ability to have a strongly typed, RDD-like experience, but with the added optimizations from Spark SQL.  We can get a more dynamic experience by using a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b02426-4f2e-4f4e-acd3-5e5562cd93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val personDF = personDS.toDF\n",
    "personDF.show\n",
    "personDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91fa194-4e3f-4117-b3fe-50d0634aa808",
   "metadata": {},
   "source": [
    "The differences here are obscured, but show up when we interrogate the types of the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6654e5b-7d2e-41dd-8006-612409de7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(f\"type(personDS.first) = ${personDS.first.getClass}\")\n",
    "println(f\"type(personDF.first) = ${personDF.first.getClass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d8c50-ba05-4cad-829b-059f70d34821",
   "metadata": {},
   "source": [
    "(Here we can see why we have to use the special incantation in the cells where we declare the case classes.  Because of the way that the Ammonite interpreter declares classes, we have to make sure `Person` is in scope.)  The Dataframe has quietly reencoded the content of the Dataset in an internal [`Row`](https://spark.apache.org/docs/3.3.1/api/scala/org/apache/spark/sql/Row.html) instance.  In fact, in the Scala implementation, `DataFrame` is just a type alias for `Dataset[Row]`.  We are able to convert a DataFrame back into a Dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b36f3-2253-4c75-8c88-7620bb78392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "personDF.as[Person].first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2ab38-a583-456b-ba74-0b2c25b02a42",
   "metadata": {},
   "source": [
    "An interesting thing, is that the Encoder system allows us to convert a Dataset or DataFrame to any case class which has a schema that is a subset of the schema of the source container.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e542de-cbc0-44d5-9912-07547321394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class NamedEntity(name: String)\n",
    "\n",
    "personDF.as[NamedEntity].first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb080d-a620-4341-b630-85d7df65f9b1",
   "metadata": {},
   "source": [
    "If we attempt to convert to a type that does not match the schema, we'll get a fairly clear error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d196d8-7d76-4b9e-9c49-ed297de1a89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Employee(name: String, title: String)\n",
    "\n",
    "try {\n",
    "    personDS.as[Employee]\n",
    "} catch {\n",
    "    case e: Exception => println(e)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22f205-ed7a-4d39-b87b-870badf67cf6",
   "metadata": {},
   "source": [
    "In a limited sense, this provides a type-safe `select` statement.\n",
    "\n",
    "We can then use the standard battery of processing functions to do tasks, such as `filter`, `map`, `reduce`, etc.  See the [Dataset documentation](https://spark.apache.org/docs/3.3.1/api/scala/org/apache/spark/sql/Dataset.html) for details.\n",
    "\n",
    "### Aggregations\n",
    "\n",
    "It's also possible to do operations that group by key, and then apply aggregation to the result.  Unlike with RDDs, one does not need to be wary of using the Dataset `groupByKey` operation.  This is due to the optimization of Spark SQL's execution engine.  Because `groupByKey` does not produce a `Dataset`, but a [`KeyValueGroupedDataset`](https://spark.apache.org/docs/3.3.1/api/scala/org/apache/spark/sql/KeyValueGroupedDataset.html), which has only aggregation-type methods associated to it, we should consider that `groupByKey` and the aggregation function that necessarily follows it are, together, playing the role of a `combineByKey`-style operation.  The optimizer works hard to limit the shuffle data volume to what is needed to make the operation work.\n",
    "\n",
    "We should look at an example of grouped aggregation.  This requires a datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7c14b-2dbc-4bb9-a60d-877f3bed9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Item(department: String, item_name: String, quantity: Int, price: Double)\n",
    "\n",
    "val inventory = spark.createDataset(Seq(\n",
    "    Item(\"produce\", \"oranges\", 20, 0.99),\n",
    "    Item(\"produce\", \"persimmons\", 12, 2.50),\n",
    "    Item(\"grocery\", \"spaghetti\", 31, 1.29),\n",
    "    Item(\"deli\", \"cheddar\", 6, 3.99),\n",
    "    Item(\"grocery\", \"chips\", 18, 2.99)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad3826-9424-43b4-939d-1e97b6b377e2",
   "metadata": {},
   "source": [
    "To do the calculation that I've got in mind, we need to bring in the `org.apache.spark.sql.functions` module, which provides a wide variety of Column functions to work with.  These are functions which take columnar elements of a Dataset/DataFrame and recombine them into new columnar elements.  To compute the value in the department, we can simply create a new Column entity.  For us, the sum of the product of quantity and price is what we want.  The `sum` is an aggregation function, but the product is a standard Column function.  So, we need a handle on the columns in the Dataset, which are inferred from the case class elements.\n",
    "\n",
    "There are a variety of ways to identify a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c0228-47a4-4a65-9e3d-89e673b22511",
   "metadata": {},
   "outputs": [],
   "source": [
    "col(\"name\")\n",
    "$\"name\"\n",
    "'name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccdbbc-b590-4aa3-9ad6-9eaf30c04383",
   "metadata": {},
   "source": [
    "The last two rely on implicit conversions that were brought in with the `import spark.implicits._` directive above.  Note that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bce9ce-dc25-4539-bc98-48ba7db7fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "$\"name\": Column\n",
    "'name: Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d1b44-c023-4fa3-97a1-904fd262541e",
   "metadata": {},
   "source": [
    "both coerce the types as desired.\n",
    "\n",
    "We can then multiply the quantity and price columns, and then aggregate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395985b5-d4ff-4543-8213-cf3d3bee14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val departmentValue = inventory.groupByKey(_.department).agg(\n",
    "    sum('quantity * 'price).as(\"value\").as[Double]\n",
    ").withColumnRenamed(\"key\", \"department\")\n",
    "departmentValue.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c4b20-549a-41c8-a7e9-2b06f5e62614",
   "metadata": {},
   "source": [
    "Keep in mind that because we are in the strongly-typed Dataset regime, we need to use a `TypedColumn` to describe the result of the `sum`.  This is what the `as[Double]` does for us (the first is to alias the column to a more intuitive name).  DataFrame operations usually can be specified with plain `Column`s.\n",
    "\n",
    "Also note that some aggregation tasks fall outside the reach of the standard functions.  In these cases, we will have to write [custom aggregation functions](https://spark.apache.org/docs/latest/sql-ref-functions-udf-aggregate.html).  This topic is out of scope for this tutorial, but best to know that it is there in case you need it.\n",
    "\n",
    "## DataFrames\n",
    "\n",
    "At some point, we are likely to want to exit this rigidly-typed regime and take advantage of the flexibility of the DataFrame type.  By doing this, we get a much more SQL-like interface.  Joins and querys take on a more familiar appearance, and we have to worry less about typing.  DataFrames are also how the DataSource infrastructure returns results, which means we will interact with them at least a little bit in most cases.\n",
    "\n",
    "### Reading data\n",
    "\n",
    "Getting data into the system with RDDs was tricky due for a couple reasons:\n",
    "1. the burden of having to read data either as text, later coercing it into a form that we can work with, or needing to use the Hadoop file system approach, which adds an additional layer of complexity; and\n",
    "2. for the lack of good, efficient representations of tabular data.\n",
    "\n",
    "The Dataset/DataFrame infrastructure fixes the latter problem, and the DataSource infrastructure fixes the former.  Spark SQL comes built in with a variety of readers: CSV, text, ORC, Parquet, JSON, JDBC, and others.  This infrastructure is extensible, so custom data sources can be created if needed.\n",
    "\n",
    "> Note: The DataSource API has been traditionally the least stable part of the Spark SQL API, so if you go this route, be mindful of the possibility of breakage during Spark version upgrades.\n",
    "\n",
    "The datasource infrastructure is available as methods hanging off `spark.read` and `<dataset>.write`.  Let's load some data to work with.  We'll opt to do a little bit of processing on an OpenStreetMap excerpt of the Isle of Man.  This has been converted into an ORC file with [osm2orc](https://github.com/mojodna/osm2orc), so we can use the ORC loading facility from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd093cf-a87e-4999-8821-21062bb90ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val windowSpec = Window.partitionBy(\"id\").orderBy(desc(\"version\"))\n",
    "\n",
    "val iom = spark.read.orc(\"data/isle-of-man.orc\")\n",
    "  .withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "  .filter('row_number === 1 && 'visible ===true)\n",
    "  .drop('row_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76156db5-2a41-42b9-b0e0-213cfd886090",
   "metadata": {},
   "source": [
    "We've done a light amount of pre-processing here to only keep the most recent live version of the OSM elements.  The window function is used to enumerate the various elements by their version.  We keep only the most recent versions if they are visible—meaning they haven't been deleted.  This operation will make sure we don't double count as we go forward.\n",
    "\n",
    "For clarity, let's look at the schema for the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7d061-36bd-4a4e-9e92-507df0b6abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iom.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f52a9-3565-47c6-b314-aa1e713b1615",
   "metadata": {},
   "source": [
    "For this example, we can attempt to measure the total length of the rivers on the Isle of Man that have been added to OSM.  In OSM parlance, this means finding the ways which have a tag named `waterway` with a value of `river`.  Looking above, we can see that the `tags` field is a Map, so we can access the `waterway` field of that map and check for the expected value.\n",
    "\n",
    "Once we have all the ways with the river tag, we need to prep for a join by exploding over the `nds` field.  Because `nds` has this odd nested struct definition, we have to unpack it as well.  We can quickly do all of these things in the following expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a4d1a-78ed-4d06-948a-b60a1abcac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wayNds = iom\n",
    "  .filter('type === \"way\" && 'tags(\"waterway\") === \"river\")\n",
    "  .select('id, posexplode('nds))\n",
    "  .select('id, 'pos, $\"col.ref\" as \"nid\")\n",
    "wayNds.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e5924-7d30-46bf-aea7-2c98f0277bc2",
   "metadata": {},
   "source": [
    "We'll also need to find all the node coordinates, indexed by their `id`, since this is what we'll join to the table above.  We'll rename the `id` column to `nid` to match the above and make the upcoming join easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0716d58-14ae-43a2-b104-078ef3b7163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val ndXYs = iom\n",
    "  .filter('type === \"node\") // && 'lat.isNotNull && 'lon.isNotNull)\n",
    "  .select('id as \"nid\", 'lat, 'lon)\n",
    "ndXYs.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990493e-4d43-4315-89e2-3bbd840ee7b6",
   "metadata": {},
   "source": [
    "Now, the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3be299-5c82-46de-ae5a-3e47a692523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wayXYs = wayNds.join(ndXYs, Seq(\"nid\")).drop('nid)\n",
    "wayXYs.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5009cf-963b-4088-a836-2393156c55e4",
   "metadata": {},
   "source": [
    "Earlier, we had used `posexplode`, which gave us the node ids in each way, indexed by their position in the `nds` array.  This way, we can reconstruct the geometry.\n",
    "\n",
    "Computing the length is relatively easily done by taking three arrays—the position index, the latitudes, and the longitudes—and reconsitituting the geometry.  We can use the Geotrellis vector facilities to construct the LineString, project it to WebMercator, where the units are roughly eqivalent to meters (modulo a latitude-based scaling factor that we'll not worry about for this simple example), and compute the length of the reprojcted geometry.  Here's the function that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd4739-c0af-4c2c-a610-9b7ea6dce7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.locationtech.geotrellis::geotrellis-vector:3.6.3`\n",
    "import geotrellis.vector._\n",
    "import geotrellis.proj4.{LatLng, WebMercator}\n",
    "\n",
    "def reconstructLineString(idx: Array[Int], lat: Array[Double], long: Array[Double]): LineString = {\n",
    "    val llCoords = idx.zip(lat.zip(long)).sortBy(_._1).map(_._2)\n",
    "    LineString(llCoords)\n",
    "}\n",
    "\n",
    "def wayLengthFn(idx: Array[Int], lat: Array[Double], long: Array[Double]): Double = \n",
    "  reconstructLineString(idx, lat, long).reproject(LatLng, WebMercator).getLength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef888f99-e3a4-468e-bc44-6646625b65c8",
   "metadata": {},
   "source": [
    "The astute observer will now pick up on the fact that `wayLengthFn` is not a Column function, and so cannot be used in a Spark SQL expression tree.  How we bridge the divide is by creating a special column type called a _user defined function_.  These are created by using the `udf` function, passing it a function.  The resulting Column will have an arity equivalent to the argument function, using the same types, and returning the same type as the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbb28e-2a80-44df-ae8e-233b110ea11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wayLength = udf(wayLengthFn(_, _, _))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0b574-f322-414f-b7ac-e546695ae6fd",
   "metadata": {},
   "source": [
    "To apply this function, we need to group by the way ID, collect the corresponding `pos`, `lat`, and `lon` fields into arrays, and then pass the results to the `wayLength` UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191774fb-3f69-49ea-91dc-ccd52ef4637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wayLengths = wayXYs \n",
    "  .groupBy('id)\n",
    "  .agg(\n",
    "    collect_list('pos) as \"pos\", \n",
    "    collect_list('lat) as \"lat\", \n",
    "    collect_list('lon) as \"lon\"\n",
    "  )\n",
    "  .select('id, wayLength('pos, 'lon, 'lat) as 'length)\n",
    "\n",
    "wayLengths.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4218f-50d1-4d4f-9613-4a012c579c84",
   "metadata": {},
   "source": [
    "To complete the obective, we can do a simple sum aggregation (which has a bit of an odd syntax), grab the single result using `first`, and since that result is packed in a `Row`, we need to unpack the value using a `get`.  This is a point at which we see the dynamic nature of a DataFrame: it doesn't know the types of its contents.  If we use a raw `get`, the return value will be of `Any` type.  We will have to know the result of our computation, and use the appropriately-typed `get` version, in this case, `getDouble`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b01054-e78a-4d3f-a39f-b43f6e57f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wayLengths\n",
    "  .agg(sum('length))\n",
    "  .first\n",
    "  .getDouble(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91acd27-8832-4bd9-bec8-6f267bb35123",
   "metadata": {},
   "source": [
    "## User-defined types\n",
    "\n",
    "At the moment, what we have is an extremely useful, SQL-like mechanism for working with structured data.  In it's native form, there's a great deal of work we can do.  However, we're missing an important feature that we had access to in RDD-land: the ability to use non-standard-typed objects in our Datasets and DataFrames.  Considering that we often use JTS Geometry instances and Geotrellis raster types in our work, this is a significant loss of function.  Consider that, in our current state, we could not make a UDF that returned Geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796ab5a-9477-4d3c-a63f-2b48a3c86ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try {\n",
    "    udf(reconstructLineString(_, _, _))\n",
    "    ()\n",
    "} catch {\n",
    "    case e: Exception => println(e)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e9b72-d81b-4a12-a973-24d1f3fcaaf1",
   "metadata": {},
   "source": [
    "Fortunately, Spark SQL also offers _user defined types_, and even more fortunately, there is a project that provides UDTs for the most important JTS and Geotrellis types, as well as a host of functions for manipulating them: [Rasterframes](https://github.com/locationtech/rasterframes).\n",
    "\n",
    "We can import this project and enable its features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6545d71-2ed7-4b9c-b0f1-436d13456c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.locationtech.rasterframes::rasterframes:0.11.1`\n",
    "import org.locationtech.rasterframes._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df774f2-2dff-41ac-adeb-1473e69904fe",
   "metadata": {},
   "source": [
    "The following line registers the custom UDTs and accompanying UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb3f44-cf9c-4b32-b304-452375897894",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.withRasterFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409bb0d-05f2-4924-afc2-4612e00fae86",
   "metadata": {},
   "source": [
    "Now, we can define the UDF successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440c631-29b8-42af-b964-768b03361781",
   "metadata": {},
   "outputs": [],
   "source": [
    "val convertToLineString = udf(reconstructLineString(_, _, _))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd59ff-97f3-4f4d-acf0-adcddb04ec6a",
   "metadata": {},
   "source": [
    "The result of this UDF can be stored in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da209e01-f856-44f2-b265-5f67b5d258f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val wayGeoms = wayXYs \n",
    "  .groupBy('id)\n",
    "  .agg(\n",
    "    collect_list('pos) as \"pos\", \n",
    "    collect_list('lat) as \"lat\", \n",
    "    collect_list('lon) as \"lon\"\n",
    "  )\n",
    "  .select('id, convertToLineString('pos, 'lon, 'lat) as 'geom)\n",
    "wayGeoms.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa17a0d-7291-4a69-8875-3fe36ed7c42e",
   "metadata": {},
   "source": [
    "Finally, we can use the provided `st_lengthSphere` function to compute the lengths and sum up as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7673aeea-0b20-4df5-a8f5-6fd43596a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.locationtech.rasterframes.functions._\n",
    "import org.locationtech.geomesa.spark.jts.st_lengthSphere\n",
    "\n",
    "wayGeoms\n",
    "  .select(st_lengthSphere('geom) as 'length)\n",
    "  .agg(sum('length))\n",
    "  .first\n",
    "  .getDouble(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdd0d9-b765-4be6-979b-7fa828b62772",
   "metadata": {},
   "source": [
    "The specific value doesn't match, but the general procedure is sound, and our interest here is to show the capabilities more than it is to get a reliable answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81822ced-d335-4e72-a2d6-e546fc7ad998",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
